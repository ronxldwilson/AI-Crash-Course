# 🧠 Retrieval-Augmented Generation (RAG) – Beginner Notes

## 🚀 What is RAG?

**RAG** stands for **Retrieval-Augmented Generation**.
It’s a way to make AI models **smarter and more accurate** by letting them **look up useful information** before answering your question.

Think of it like this:

> Instead of asking an AI to remember everything, RAG lets it **search a document library** and then use what it finds to give you a better answer.

---

## 🤔 Why Use RAG?

Language models (like ChatGPT or LLaMA) are trained on tons of text, but:

* They **forget recent or private info**
* They sometimes **make up facts** (this is called hallucination)
* They **can’t know your company’s internal documents**

**RAG solves this by letting the AI search real data first**, then use that to generate answers.

---

## 🔍 How Does RAG Work?

### In simple steps:

1. **You ask a question.**

   > e.g. “What are the symptoms of dengue?”

2. **RAG searches your document collection** (like PDFs, notes, websites).

   > Finds 2–3 documents that mention dengue.

3. **It gives those documents to the AI model**, along with your question.

4. **The AI uses both the documents and your question** to write an answer.

---

## 🛠️ What Do You Need for RAG?

### 1. **Documents to Search**

* Can be any text: PDFs, websites, Notion pages, etc.

### 2. **Embeddings**

* This means converting each document into numbers that capture its meaning.
* Used to help search similar documents.

### 3. **Vector Database**

* Stores the embeddings and helps you find similar content.
* Popular tools: **FAISS**, **Chroma**, **Pinecone**

### 4. **Retriever**

* This is the part that searches your documents.
* It finds the **top K** most relevant chunks.

### 5. **LLM (Language Model)**

* This is the AI that answers your question using the documents.
* Examples: **GPT**, **LLaMA**, **Mistral**

---

## 💡 Example of How RAG Works

Let’s say your document contains:

> “Dengue causes high fever, severe headache, pain behind the eyes, joint and muscle pain, and skin rash.”

### You ask:

**“What are common symptoms of dengue?”**

🔹 The retriever finds the sentence above.
🔹 The AI reads it.
🔹 The answer it gives:

> “Common symptoms of dengue include high fever, headache, pain behind the eyes, joint pain, and skin rash.”

🎉 It used real info from your documents!

---

## 🧪 Prompt Example

Here’s what the AI might be shown behind the scenes:

```
Context:
"Dengue causes high fever, severe headache, pain behind the eyes, joint and muscle pain, and skin rash."

Question:
What are the symptoms of dengue?

Answer:
```

---

## 🧰 Tools That Help You Build RAG

| Tool                | What it does                         |
| ------------------- | ------------------------------------ |
| **LangChain**       | Combines all parts together          |
| **LlamaIndex**      | Easy RAG with documents              |
| **FAISS**           | Free tool to store/search embeddings |
| **Chroma**          | Another vector database              |
| **Hugging Face**    | Models + embeddings                  |
| **OpenAI / Ollama** | For the AI model (GPT, LLaMA etc.)   |

---

## 🔄 RAG vs Fine-Tuning (Beginner View)

| RAG                     | Fine-Tuning                    |
| ----------------------- | ------------------------------ |
| Searches documents      | Learns new info in its weights |
| Easy to update          | Takes lots of compute          |
| No retraining needed    | Needs retraining               |
| Can explain its answers | May hallucinate facts          |

---

## ⚠️ Things to Watch Out For

* If your documents are messy, AI might give bad answers.
* If the retriever doesn’t find good info, the AI won’t help.
* Too many documents = too long input (token limit)
* Keep context short, clear, and relevant.

---

## ✅ Summary

* RAG = Retrieval + Generation
* Great for answering questions using your own data
* Helps reduce hallucination
* Easy to update (just add or change documents)
* Powered by embedding models, retrievers, and LLMs


## 🎯 Why Use RAG?

| Benefit                      | Description                                                              |
| ---------------------------- | ------------------------------------------------------------------------ |
| 📚 Access external knowledge | LLMs have a cutoff; RAG enables access to up-to-date or proprietary data |
| 💡 Reduces hallucination     | Grounding responses in real data helps avoid made-up facts               |
| 🧩 Domain-specific QA        | Makes LLMs effective for enterprise knowledge, legal, medical, etc.      |
| 🧠 Memory efficiency         | Avoids needing to fine-tune LLMs with large knowledge corpora            |

---

## 🏗️ How RAG Works (Architecture)

### 🔁 Core Workflow:

1. **User Query → Retriever**

   * Extract top-k relevant documents from a corpus using vector similarity.
2. **Retriever Output → Generator**

   * Feed retrieved documents + query into an LLM as context.
3. **Generator → Answer**

   * Model generates a response grounded in the retrieved data.

```
[Query]
   ↓
[Retriever (Dense/Hybrid)]
   ↓
[Relevant Passages]
   ↓
[Prompt + LLM]
   ↓
[Generated Answer]
```

---

## 🔍 Components of RAG

### 1. **Retriever**

#### 🔎 Dense Retriever:

* Uses embeddings + vector similarity (cosine, dot product)
* Tools: `FAISS`, `Weaviate`, `Qdrant`, `Chroma`, `Pinecone`

#### 🔀 Hybrid Retriever:

* Mix of dense + sparse (BM25) methods
* Often improves recall

#### Embedding Models:

* Sentence Transformers (`all-MiniLM`, `bge-small`)
* OpenAI Embeddings (`text-embedding-ada-002`)
* Cohere, HuggingFace, etc.

---

### 2. **Generator**

* Large language model that consumes the query and documents

* Types:

  * Decoder-only (e.g., GPT, LLaMA, Mistral)
  * Encoder-decoder (e.g., FLAN-T5, BART)

* Input Format (example):

```text
Context:
- Document 1: ...
- Document 2: ...

Question: What are the symptoms of dengue?
```

---

## 🧠 RAG Variants

| Type                       | Description                                                                     |
| -------------------------- | ------------------------------------------------------------------------------- |
| **Standard RAG**           | Retrieve top-k docs and pass as input to LLM                                    |
| **RAG with Reranker**      | Use cross-encoder to rerank retrievals                                          |
| **RAG with Feedback Loop** | Adjust retriever based on model's performance (e.g., RAGAS, Feedback-Augmented) |
| **Multi-hop RAG**          | Retrieve in stages (first entity → related topics)                              |

---

## ⚙️ RAG Stack (Typical Tools)

| Layer         | Tool                                              |
| ------------- | ------------------------------------------------- |
| Embedding     | `sentence-transformers`, OpenAI API, Cohere       |
| Vector DB     | `FAISS`, `Chroma`, `Qdrant`, `Pinecone`           |
| Retriever     | `LangChain`, `LlamaIndex`, custom search          |
| Generator     | `transformers`, `OpenAI`, `vLLM`, `Ollama`        |
| Orchestration | `LangChain`, `LlamaIndex`, `Haystack`, `RAGStack` |
| UI            | Streamlit, Gradio, custom frontend                |

---

## 🔐 Prompt Template Example (RAG)

```text
You are a helpful assistant. Use the following context to answer the question.

Context:
{{retrieved_docs}}

Question: {{user_query}}

Answer:
```

You can also add:

* `-- Do not answer if context is irrelevant`
* `-- Cite document snippets`

---

## 📦 Example Code (LangChain-style)

```python
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

# Setup retriever
embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
db = FAISS.load_local("faiss_index", embedding)
retriever = db.as_retriever()

# Load generator model
llm = HuggingFacePipeline.from_model_id("tiiuae/falcon-7b-instruct", task="text-generation")

# RAG pipeline
qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type="stuff")
qa.run("What is the role of mitochondria?")
```

---

## 📈 Evaluation of RAG

| Metric              | Description                                                                            |
| ------------------- | -------------------------------------------------------------------------------------- |
| **EM / F1 / ROUGE** | If ground-truth answers are available                                                  |
| **Faithfulness**    | Is the output grounded in retrieved content?                                           |
| **Relevance**       | Are retrieved docs useful?                                                             |
| **Latency**         | Speed of retrieval + generation                                                        |
| **RAGAS**           | Open-source framework for RAG eval (faithfulness, answer relevance, context precision) |

---

## ⚠️ Challenges & Limitations

* ❌ Retrieval mismatch or irrelevant documents
* ❌ Long documents may exceed token limits
* ❌ Generator may ignore context (especially if not well formatted)
* ❌ Outdated or noisy corpus
* ❌ Requires infra (embedding, DB, LLM)

---

## 🧩 RAG vs Fine-Tuning

| Aspect             | RAG                   | Fine-Tuning                          |
| ------------------ | --------------------- | ------------------------------------ |
| Data freshness     | ✅ Dynamic             | ❌ Static                             |
| Cost               | ✅ Cheaper             | ❌ Costly compute                     |
| Domain flexibility | ✅ Easy to swap corpus | ❌ Needs retraining                   |
| Accuracy           | ✅ Grounded            | ✅ Customized, but risk hallucination |
| Control            | ❌ Prompt-only         | ✅ Weight-level                       |


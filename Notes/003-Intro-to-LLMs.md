# Introduction to LLMs

## What is Large Language Model
- AI Model trained to understand and generate language
- Based on transformer architecture
- Learn from massive text database
    - Common crawl database - scrape the entire internet, and teach it everything it can learn
- 2025 models are multimodal and context-aware
    - it can do more than text, it can do images, audio and others

## Key concepts
- Parameters: Billions to Trillions 
- Zero-shot, few-shot, and fine tuning 
- Chain of thought reasoning and world models
- Mixture of experts improves efficiency

## How Do LLMs Work?
- Predict next token based on context 
- Self attention & positional encoding 
- Embeddings turn text into vectors 
- Large scale training with human feedback

## Transformer Architecture
- Core of LLMs: Self attention layers
- Enables parallel processing of text
- Modern variants are optimized for multimodalities

## Prompting and context windows
- LLMs operate on prompts
- Few Shot prompting enhances performance
- 2025 models support > 1M token windows

## Training and Fine Tuning
- Pretraining on internet scale text
- Reinforcement learning from human feedback
- fine tuned for specific tasks
- Retrieval Augmneted Generation (RAG) for grounding 

## Application of LLMs
- Writing and Summarization
- Customer service and chatbots
- Code generation and debugging 
- Medical/Legal assisstant and tutoring









# ğŸ§  Retrieval-Augmented Generation (RAG) â€“ Beginner Notes

## ğŸš€ What is RAG?

**RAG** stands for **Retrieval-Augmented Generation**.
Itâ€™s a way to make AI models **smarter and more accurate** by letting them **look up useful information** before answering your question.

Think of it like this:

> Instead of asking an AI to remember everything, RAG lets it **search a document library** and then use what it finds to give you a better answer.

---

## ğŸ¤” Why Use RAG?

Language models (like ChatGPT or LLaMA) are trained on tons of text, but:

* They **forget recent or private info**
* They sometimes **make up facts** (this is called hallucination)
* They **canâ€™t know your companyâ€™s internal documents**

**RAG solves this by letting the AI search real data first**, then use that to generate answers.

---

## ğŸ” How Does RAG Work?

### In simple steps:

1. **You ask a question.**

   > e.g. â€œWhat are the symptoms of dengue?â€

2. **RAG searches your document collection** (like PDFs, notes, websites).

   > Finds 2â€“3 documents that mention dengue.

3. **It gives those documents to the AI model**, along with your question.

4. **The AI uses both the documents and your question** to write an answer.

---

## ğŸ› ï¸ What Do You Need for RAG?

### 1. **Documents to Search**

* Can be any text: PDFs, websites, Notion pages, etc.

### 2. **Embeddings**

* This means converting each document into numbers that capture its meaning.
* Used to help search similar documents.

### 3. **Vector Database**

* Stores the embeddings and helps you find similar content.
* Popular tools: **FAISS**, **Chroma**, **Pinecone**

### 4. **Retriever**

* This is the part that searches your documents.
* It finds the **top K** most relevant chunks.

### 5. **LLM (Language Model)**

* This is the AI that answers your question using the documents.
* Examples: **GPT**, **LLaMA**, **Mistral**

---

## ğŸ’¡ Example of How RAG Works

Letâ€™s say your document contains:

> â€œDengue causes high fever, severe headache, pain behind the eyes, joint and muscle pain, and skin rash.â€

### You ask:

**â€œWhat are common symptoms of dengue?â€**

ğŸ”¹ The retriever finds the sentence above.
ğŸ”¹ The AI reads it.
ğŸ”¹ The answer it gives:

> â€œCommon symptoms of dengue include high fever, headache, pain behind the eyes, joint pain, and skin rash.â€

ğŸ‰ It used real info from your documents!

---

## ğŸ§ª Prompt Example

Hereâ€™s what the AI might be shown behind the scenes:

```
Context:
"Dengue causes high fever, severe headache, pain behind the eyes, joint and muscle pain, and skin rash."

Question:
What are the symptoms of dengue?

Answer:
```

---

## ğŸ§° Tools That Help You Build RAG

| Tool                | What it does                         |
| ------------------- | ------------------------------------ |
| **LangChain**       | Combines all parts together          |
| **LlamaIndex**      | Easy RAG with documents              |
| **FAISS**           | Free tool to store/search embeddings |
| **Chroma**          | Another vector database              |
| **Hugging Face**    | Models + embeddings                  |
| **OpenAI / Ollama** | For the AI model (GPT, LLaMA etc.)   |

---

## ğŸ”„ RAG vs Fine-Tuning (Beginner View)

| RAG                     | Fine-Tuning                    |
| ----------------------- | ------------------------------ |
| Searches documents      | Learns new info in its weights |
| Easy to update          | Takes lots of compute          |
| No retraining needed    | Needs retraining               |
| Can explain its answers | May hallucinate facts          |

---

## âš ï¸ Things to Watch Out For

* If your documents are messy, AI might give bad answers.
* If the retriever doesnâ€™t find good info, the AI wonâ€™t help.
* Too many documents = too long input (token limit)
* Keep context short, clear, and relevant.

---

## âœ… Summary

* RAG = Retrieval + Generation
* Great for answering questions using your own data
* Helps reduce hallucination
* Easy to update (just add or change documents)
* Powered by embedding models, retrievers, and LLMs


## ğŸ¯ Why Use RAG?

| Benefit                      | Description                                                              |
| ---------------------------- | ------------------------------------------------------------------------ |
| ğŸ“š Access external knowledge | LLMs have a cutoff; RAG enables access to up-to-date or proprietary data |
| ğŸ’¡ Reduces hallucination     | Grounding responses in real data helps avoid made-up facts               |
| ğŸ§© Domain-specific QA        | Makes LLMs effective for enterprise knowledge, legal, medical, etc.      |
| ğŸ§  Memory efficiency         | Avoids needing to fine-tune LLMs with large knowledge corpora            |

---

## ğŸ—ï¸ How RAG Works (Architecture)

### ğŸ” Core Workflow:

1. **User Query â†’ Retriever**

   * Extract top-k relevant documents from a corpus using vector similarity.
2. **Retriever Output â†’ Generator**

   * Feed retrieved documents + query into an LLM as context.
3. **Generator â†’ Answer**

   * Model generates a response grounded in the retrieved data.

```
[Query]
   â†“
[Retriever (Dense/Hybrid)]
   â†“
[Relevant Passages]
   â†“
[Prompt + LLM]
   â†“
[Generated Answer]
```

---

## ğŸ” Components of RAG

### 1. **Retriever**

#### ğŸ” Dense Retriever:

* Uses embeddings + vector similarity (cosine, dot product)
* Tools: `FAISS`, `Weaviate`, `Qdrant`, `Chroma`, `Pinecone`

#### ğŸ”€ Hybrid Retriever:

* Mix of dense + sparse (BM25) methods
* Often improves recall

#### Embedding Models:

* Sentence Transformers (`all-MiniLM`, `bge-small`)
* OpenAI Embeddings (`text-embedding-ada-002`)
* Cohere, HuggingFace, etc.

---

### 2. **Generator**

* Large language model that consumes the query and documents

* Types:

  * Decoder-only (e.g., GPT, LLaMA, Mistral)
  * Encoder-decoder (e.g., FLAN-T5, BART)

* Input Format (example):

```text
Context:
- Document 1: ...
- Document 2: ...

Question: What are the symptoms of dengue?
```

---

## ğŸ§  RAG Variants

| Type                       | Description                                                                     |
| -------------------------- | ------------------------------------------------------------------------------- |
| **Standard RAG**           | Retrieve top-k docs and pass as input to LLM                                    |
| **RAG with Reranker**      | Use cross-encoder to rerank retrievals                                          |
| **RAG with Feedback Loop** | Adjust retriever based on model's performance (e.g., RAGAS, Feedback-Augmented) |
| **Multi-hop RAG**          | Retrieve in stages (first entity â†’ related topics)                              |

---

## âš™ï¸ RAG Stack (Typical Tools)

| Layer         | Tool                                              |
| ------------- | ------------------------------------------------- |
| Embedding     | `sentence-transformers`, OpenAI API, Cohere       |
| Vector DB     | `FAISS`, `Chroma`, `Qdrant`, `Pinecone`           |
| Retriever     | `LangChain`, `LlamaIndex`, custom search          |
| Generator     | `transformers`, `OpenAI`, `vLLM`, `Ollama`        |
| Orchestration | `LangChain`, `LlamaIndex`, `Haystack`, `RAGStack` |
| UI            | Streamlit, Gradio, custom frontend                |

---

## ğŸ” Prompt Template Example (RAG)

```text
You are a helpful assistant. Use the following context to answer the question.

Context:
{{retrieved_docs}}

Question: {{user_query}}

Answer:
```

You can also add:

* `-- Do not answer if context is irrelevant`
* `-- Cite document snippets`

---

## ğŸ“¦ Example Code (LangChain-style)

```python
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

# Setup retriever
embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
db = FAISS.load_local("faiss_index", embedding)
retriever = db.as_retriever()

# Load generator model
llm = HuggingFacePipeline.from_model_id("tiiuae/falcon-7b-instruct", task="text-generation")

# RAG pipeline
qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type="stuff")
qa.run("What is the role of mitochondria?")
```

---

## ğŸ“ˆ Evaluation of RAG

| Metric              | Description                                                                            |
| ------------------- | -------------------------------------------------------------------------------------- |
| **EM / F1 / ROUGE** | If ground-truth answers are available                                                  |
| **Faithfulness**    | Is the output grounded in retrieved content?                                           |
| **Relevance**       | Are retrieved docs useful?                                                             |
| **Latency**         | Speed of retrieval + generation                                                        |
| **RAGAS**           | Open-source framework for RAG eval (faithfulness, answer relevance, context precision) |

---

## âš ï¸ Challenges & Limitations

* âŒ Retrieval mismatch or irrelevant documents
* âŒ Long documents may exceed token limits
* âŒ Generator may ignore context (especially if not well formatted)
* âŒ Outdated or noisy corpus
* âŒ Requires infra (embedding, DB, LLM)

---

## ğŸ§© RAG vs Fine-Tuning

| Aspect             | RAG                   | Fine-Tuning                          |
| ------------------ | --------------------- | ------------------------------------ |
| Data freshness     | âœ… Dynamic             | âŒ Static                             |
| Cost               | âœ… Cheaper             | âŒ Costly compute                     |
| Domain flexibility | âœ… Easy to swap corpus | âŒ Needs retraining                   |
| Accuracy           | âœ… Grounded            | âœ… Customized, but risk hallucination |
| Control            | âŒ Prompt-only         | âœ… Weight-level                       |

